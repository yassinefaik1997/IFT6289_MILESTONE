{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Data Loading and Splitting\n",
    "# -----------------------------\n",
    "folder_path = os.path.join(\"stocks\", \"datasets\", \"dj30\", \"raw\", \"*.csv\")\n",
    "csv_files = glob.glob(folder_path)\n",
    "data_frames = []\n",
    "\n",
    "for file in csv_files:\n",
    "    symbol = os.path.splitext(os.path.basename(file))[0]\n",
    "    print(f\"Processing {symbol} from {file}...\")\n",
    "    df = pd.read_csv(file, parse_dates=['Date'])\n",
    "    mask = (df['Date'] >= '2010-01-01') & (df['Date'] <= '2018-12-21')\n",
    "    df = df.loc[mask]\n",
    "    df = df[['Date', 'Adj Close']].set_index('Date')\n",
    "    df.rename(columns={'Adj Close': symbol}, inplace=True)\n",
    "    data_frames.append(df)\n",
    "\n",
    "merged_df = pd.concat(data_frames, axis=1)\n",
    "merged_df.sort_index(inplace=True)\n",
    "\n",
    "# Split data:\n",
    "train_data = merged_df.loc['2010-01-01':'2016-01-01']\n",
    "test_data  = merged_df.loc['2016-02-01':'2018-02-01']\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Calibration on Training Data (for Simulation)\n",
    "# -----------------------------\n",
    "train_log_returns = np.log(train_data / train_data.shift(1)).dropna()\n",
    "window_size = 120\n",
    "correlation_matrices = []\n",
    "for start in range(0, len(train_log_returns) - window_size + 1, window_size):\n",
    "    window_data = train_log_returns.iloc[start:start+window_size]\n",
    "    corr_matrix = window_data.corr().values\n",
    "    correlation_matrices.append(corr_matrix)\n",
    "\n",
    "n_assets = train_log_returns.shape[1]\n",
    "features = [mat[np.triu_indices(n_assets, k=1)] for mat in correlation_matrices]\n",
    "features = np.array(features)\n",
    "\n",
    "Z = linkage(features, method='ward')\n",
    "n_clusters = 4\n",
    "clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "rep_corr_matrices = []\n",
    "for cluster_id in range(1, n_clusters + 1):\n",
    "    indices = np.where(clusters == cluster_id)[0]\n",
    "    avg_corr = np.mean([correlation_matrices[i] for i in indices], axis=0)\n",
    "    rep_corr_matrices.append(avg_corr)\n",
    "\n",
    "annual_factor = 252\n",
    "annual_drift = train_log_returns.mean() * annual_factor\n",
    "annual_vol = train_log_returns.std() * np.sqrt(annual_factor)\n",
    "mu = annual_drift.values    \n",
    "sigmas = annual_vol.values\n",
    "\n",
    "S0s = train_data.iloc[-1].values\n",
    "\n",
    "T = 1       # 1 year\n",
    "dt = 1 / 252\n",
    "n_steps = int(T / dt)\n",
    "\n",
    "def simulate_correlated_prices(S0s, mu, sigmas, T, dt, corr_matrix):\n",
    "    n_assets = len(S0s)\n",
    "    n_steps = int(T / dt)\n",
    "    prices = np.zeros((n_steps + 1, n_assets))\n",
    "    prices[0] = S0s\n",
    "    L = np.linalg.cholesky(corr_matrix)\n",
    "    for t in range(1, n_steps + 1):\n",
    "        Z = np.random.normal(0, 1, size=n_assets)\n",
    "        correlated_Z = L @ Z\n",
    "        prices[t] = prices[t-1] * np.exp((mu - 0.5 * sigmas**2) * dt + sigmas * np.sqrt(dt) * correlated_Z)\n",
    "    return prices\n",
    "\n",
    "def simulate_multiple_paths(S0s, mu, sigmas, T, dt, corr_matrix, num_paths=1000):\n",
    "    sims = []\n",
    "    for _ in range(num_paths):\n",
    "        sim = simulate_correlated_prices(S0s, mu, sigmas, T, dt, corr_matrix)\n",
    "        sims.append(sim)\n",
    "    return np.array(sims)\n",
    "\n",
    "all_simulations = {}\n",
    "num_paths = 1000\n",
    "for idx, corr in enumerate(rep_corr_matrices):\n",
    "    sims = simulate_multiple_paths(S0s, mu, sigmas, T, dt, corr, num_paths=num_paths)\n",
    "    all_simulations[idx] = sims\n",
    "\n",
    "combined_simulations = np.concatenate(list(all_simulations.values()), axis=0)\n",
    "dummy_dates = pd.date_range(start=\"2018-01-01\", periods=combined_simulations.shape[1], freq=\"B\")\n",
    "asset_names = [f\"Stock{i+1}\" for i in range(combined_simulations.shape[2])]\n",
    "train_episode_df = pd.DataFrame(combined_simulations[np.random.choice(combined_simulations.shape[0])],\n",
    "                                index=dummy_dates, columns=asset_names)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. RL Framework (Environment, Agent, Training)\n",
    "# -----------------------------\n",
    "class PortfolioEnv:\n",
    "    def __init__(self, price_data, window_obs=60, window_state=120):\n",
    "        self.price_data = price_data.reset_index(drop=True)\n",
    "        self.window_obs = window_obs\n",
    "        self.window_state = window_state\n",
    "        self.n_assets = price_data.shape[1]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = max(self.window_obs, self.window_state)\n",
    "        self.done = False\n",
    "        self.portfolio_value = 1.0\n",
    "        self.history = [self.portfolio_value]\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.weights = action\n",
    "        current_prices = self.price_data.iloc[self.current_step].values\n",
    "        next_prices = self.price_data.iloc[self.current_step+1].values\n",
    "        asset_returns = (next_prices / current_prices) - 1\n",
    "        portfolio_return = np.dot(self.weights, asset_returns)\n",
    "        self.portfolio_value *= (1 + portfolio_return)\n",
    "        self.history.append(self.portfolio_value)\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.price_data)-1:\n",
    "            self.done = True\n",
    "        reward = portfolio_return\n",
    "        return self._get_state(), reward, self.done, {}\n",
    "    \n",
    "    def _get_state(self):\n",
    "        obs_data = self.price_data.iloc[self.current_step - self.window_obs:self.current_step]\n",
    "        obs_returns = np.log(obs_data / obs_data.shift(1)).dropna().values.T\n",
    "        if obs_returns.shape[1] < self.window_obs:\n",
    "            pad = np.zeros((self.n_assets, self.window_obs - obs_returns.shape[1]))\n",
    "            obs_returns = np.hstack((obs_returns, pad))\n",
    "        hist = np.array(self.history)\n",
    "        if len(hist) < self.window_state+1:\n",
    "            state_data = np.log(hist[1:] / hist[:-1])\n",
    "            state_data = np.pad(state_data, (self.window_state - len(state_data), 0), 'constant')\n",
    "        else:\n",
    "            window_hist = hist[-(self.window_state+1):]\n",
    "            state_data = np.log(window_hist[1:] / window_hist[:-1])\n",
    "        state_data = state_data[np.newaxis, :]\n",
    "        obs_tensor = torch.tensor(obs_returns, dtype=torch.float32).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(state_data, dtype=torch.float32).unsqueeze(0)\n",
    "        return (obs_tensor, state_tensor)\n",
    "    \n",
    "    def render(self):\n",
    "        plt.plot(self.history)\n",
    "        plt.title(\"Portfolio Value\")\n",
    "        plt.show()\n",
    "\n",
    "def generate_actions(n_assets, n_actions=50):\n",
    "    actions = []\n",
    "    for _ in range(n_actions):\n",
    "        w = np.random.rand(n_assets)\n",
    "        w = w / w.sum()\n",
    "        actions.append(w)\n",
    "    return np.array(actions)\n",
    "\n",
    "actions = generate_actions(train_episode_df.shape[1], n_actions=50)\n",
    "\n",
    "class RLPortfolioManager(nn.Module):\n",
    "    def __init__(self, obs_channels, obs_length, state_channels, state_length, num_actions):\n",
    "        super(RLPortfolioManager, self).__init__()\n",
    "        self.obs_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=obs_channels, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_obs_length = obs_length - 4\n",
    "        self.fc_obs = nn.Linear(32 * conv_obs_length, 128)\n",
    "        \n",
    "        self.state_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=state_channels, out_channels=16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_state_length = state_length - 4\n",
    "        self.fc_state = nn.Linear(16 * conv_state_length, 128)\n",
    "        \n",
    "        self.fc_combined = nn.Linear(128 + 128, 128)\n",
    "        self.policy_head = nn.Linear(128, num_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, observation, state):\n",
    "        obs_features = self.obs_conv(observation)\n",
    "        obs_features = obs_features.view(obs_features.size(0), -1)\n",
    "        obs_features = F.relu(self.fc_obs(obs_features))\n",
    "        \n",
    "        state_features = self.state_conv(state)\n",
    "        state_features = state_features.view(state_features.size(0), -1)\n",
    "        state_features = F.relu(self.fc_state(state_features))\n",
    "        \n",
    "        combined = torch.cat([obs_features, state_features], dim=1)\n",
    "        combined = F.relu(self.fc_combined(combined))\n",
    "        policy_logits = self.policy_head(combined)\n",
    "        value = self.value_head(combined)\n",
    "        return policy_logits, value\n",
    "\n",
    "obs_channels = train_episode_df.shape[1]\n",
    "obs_length = 60\n",
    "state_channels = 1\n",
    "state_length = 120\n",
    "num_actions = actions.shape[0]\n",
    "\n",
    "policy_net = RLPortfolioManager(obs_channels, obs_length, state_channels, state_length, num_actions)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "def train_agent(policy_net, optimizer, actions, combined_simulations, num_episodes=500, window_obs=60, window_state=120, gamma=0.99):\n",
    "    for ep in range(num_episodes):\n",
    "        episode_idx = np.random.choice(combined_simulations.shape[0])\n",
    "        dummy_dates = pd.date_range(start=\"2018-01-01\", periods=combined_simulations.shape[1], freq=\"B\")\n",
    "        asset_names = [f\"Stock{i+1}\" for i in range(combined_simulations.shape[2])]\n",
    "        train_episode_df = pd.DataFrame(combined_simulations[episode_idx],\n",
    "                                        index=dummy_dates, columns=asset_names)\n",
    "        \n",
    "        env = PortfolioEnv(train_episode_df, window_obs=window_obs, window_state=window_state)\n",
    "        state = env.reset()\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            obs, port_state = state\n",
    "            policy_logits, value = policy_net(obs, port_state)\n",
    "            probs = F.softmax(policy_logits, dim=1)\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action_idx = m.sample()\n",
    "            log_prob = m.log_prob(action_idx)\n",
    "            value = value.squeeze(1)\n",
    "            action = actions[action_idx.item()]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        values = torch.stack(values).squeeze(1)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = returns - values.detach()\n",
    "        policy_loss = -(log_probs * advantage).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Episode {ep+1}/{num_episodes}, Loss: {loss.item():.4f}, Total Reward: {np.sum(rewards):.4f}\")\n",
    "    \n",
    "    return policy_net\n",
    "\n",
    "trained_policy = train_agent(policy_net, optimizer, actions, combined_simulations, num_episodes=100)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Testing on Out-of-Sample Data (2016-02-01 to 2018-02-01)\n",
    "# -----------------------------\n",
    "env_test = PortfolioEnv(test_data, window_obs=60, window_state=120)\n",
    "\n",
    "def test_agent(env, policy_net, actions):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    test_rewards = []\n",
    "    while not done:\n",
    "        obs, port_state = state\n",
    "        with torch.no_grad():\n",
    "            policy_logits, _ = policy_net(obs, port_state)\n",
    "            probs = F.softmax(policy_logits, dim=1)\n",
    "        action_idx = torch.argmax(probs, dim=1)\n",
    "        action = actions[action_idx.item()]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "    total_return = np.prod([1 + r for r in test_rewards])\n",
    "    return total_return\n",
    "\n",
    "rl_test_return = test_agent(env_test, trained_policy, actions)\n",
    "print(\"RL test return:\", rl_test_return)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Benchmark Evaluation on Fixed Out-of-Sample Intervals\n",
    "# -----------------------------\n",
    "# Define evaluation intervals entirely within the test period.\n",
    "intervals = [(\"2016-02-01\", \"2017-01-01\"),\n",
    "             (\"2017-01-01\", \"2018-02-01\")]\n",
    "\n",
    "def sharpe_ratio(rewards, risk_free_rate=0.04, periods_per_year=252):\n",
    "    rewards = np.array(rewards)\n",
    "    mean_return = np.mean(rewards)\n",
    "    std_return = np.std(rewards)\n",
    "    if std_return == 0:\n",
    "        return np.nan\n",
    "    return mean_return / std_return * np.sqrt(periods_per_year)\n",
    "\n",
    "def test_agent_with_rewards(env, policy_net, actions):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        obs, port_state = state\n",
    "        with torch.no_grad():\n",
    "            policy_logits, _ = policy_net(obs, port_state)\n",
    "            probs = F.softmax(policy_logits, dim=1)\n",
    "        action_idx = torch.argmax(probs, dim=1)\n",
    "        action = actions[action_idx.item()]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    total_return = np.prod([1 + r for r in rewards])\n",
    "    return total_return, rewards\n",
    "\n",
    "def simulate_benchmark(env, weights):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        state, reward, done, _ = env.step(weights)\n",
    "        rewards.append(reward)\n",
    "    total_return = np.prod([1 + r for r in rewards])\n",
    "    return total_return, rewards\n",
    "\n",
    "def optimize_portfolio(mu, cov):\n",
    "    n = len(mu)\n",
    "    w0 = np.ones(n) / n\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1) for _ in range(n)]\n",
    "    def neg_sharpe(w, mu, cov):\n",
    "        port_return = np.dot(w, mu)\n",
    "        port_std = np.sqrt(np.dot(w, np.dot(cov, w)))\n",
    "        return -port_return / port_std if port_std > 0 else 1e6\n",
    "    result = minimize(neg_sharpe, w0, args=(mu, cov), bounds=bounds, constraints=constraints)\n",
    "    return result.x if result.success else w0\n",
    "\n",
    "results = {}\n",
    "\n",
    "for start, end in intervals:\n",
    "    interval_df = test_data.loc[start:end].copy()\n",
    "    window_obs = 60\n",
    "    window_state = 120\n",
    "    if len(interval_df) < (window_obs + 1):\n",
    "        print(f\"Interval {start} to {end} is too short. Skipping.\")\n",
    "        continue\n",
    "    env_interval = PortfolioEnv(interval_df, window_obs=window_obs, window_state=window_state)\n",
    "    \n",
    "    # RL Agent evaluation:\n",
    "    rl_total_return, rl_rewards = test_agent_with_rewards(env_interval, trained_policy, actions)\n",
    "    rl_sharpe = sharpe_ratio(rl_rewards)\n",
    "    \n",
    "    # MVO Benchmark:\n",
    "    returns = np.log(interval_df / interval_df.shift(1)).dropna()\n",
    "    mu_interval = returns.mean().values\n",
    "    cov_interval = returns.cov().values\n",
    "    mvo_weights = optimize_portfolio(mu_interval, cov_interval)\n",
    "    mvo_total_return, mvo_rewards = simulate_benchmark(env_interval, mvo_weights)\n",
    "    mvo_sharpe = sharpe_ratio(mvo_rewards)\n",
    "    \n",
    "    # Equal Weights Benchmark:\n",
    "    n_assets = interval_df.shape[1]\n",
    "    equal_weights = np.ones(n_assets) / n_assets\n",
    "    equal_total_return, equal_rewards = simulate_benchmark(env_interval, equal_weights)\n",
    "    equal_sharpe = sharpe_ratio(equal_rewards)\n",
    "    \n",
    "    results[(start, end)] = {\n",
    "        \"RL\": {\"Total Return\": rl_total_return, \"Sharpe\": rl_sharpe},\n",
    "        \"MVO\": {\"Total Return\": mvo_total_return, \"Sharpe\": mvo_sharpe},\n",
    "        \"Equal_weights\": {\"Total Return\": equal_total_return, \"Sharpe\": equal_sharpe}\n",
    "    }\n",
    "    \n",
    "    print(f\"Interval {start} to {end}:\")\n",
    "    print(f\"  RL            - Total Return: {rl_total_return:.4f}, Sharpe Ratio: {rl_sharpe:.4f}\")\n",
    "    print(f\"  MVO           - Total Return: {mvo_total_return:.4f}, Sharpe Ratio: {mvo_sharpe:.4f}\")\n",
    "    print(f\"  Equal Weights - Total Return: {equal_total_return:.4f}, Sharpe Ratio: {equal_sharpe:.4f}\")\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
